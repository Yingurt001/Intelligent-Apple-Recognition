{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Q1: Apple Detection and Counting\n",
    "\n",
    "This notebook implements basic apple detection and counting using computer vision techniques.\n",
    "\n",
    "## Features:\n",
    "1. **Image Preprocessing**: Gaussian filtering, color enhancement  \n",
    "2. **Apple Detection**: Color-based segmentation and morphological operations\n",
    "3. **Apple Counting**: Contour detection and counting algorithm\n",
    "4. **Result Visualization**: Display detected apples with bounding boxes\n",
    "\n",
    "## Data Source:\n",
    "- Input: `data/Attachment 1/` - 200 apple orchard images (270Ã—180 pixels)\n",
    "- Output: Processed images with apple count and locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f503f28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: data/Attachment 1\n",
      "Output directory: results/Q1_results\n",
      "Available images: 0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Set up data paths\n",
    "DATA_DIR = Path(\"data\")\n",
    "INPUT_DIR = DATA_DIR / \"Attachment 1\"\n",
    "OUTPUT_DIR = Path(\"results/Q1_results\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input directory: {INPUT_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Available images: {len(list(INPUT_DIR.glob('*.jpg')))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470fe7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Image Processing Functions\n",
    "\n",
    "def read_image(image_path):\n",
    "    \"\"\"Read and return an image from the specified path.\"\"\"\n",
    "    img = cv2.imread(str(image_path), cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image from {image_path}\")\n",
    "    return img\n",
    "\n",
    "def gaussian_blur(image, kernel_size=(5, 5), sigma=1.5):\n",
    "    \"\"\"Apply Gaussian blur to reduce noise.\"\"\"\n",
    "    return cv2.GaussianBlur(image, kernel_size, sigma)\n",
    "\n",
    "def convert_to_gray(image):\n",
    "    \"\"\"Convert image to grayscale.\"\"\"\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def create_binary_threshold(gray_image):\n",
    "    \"\"\"Create binary threshold using Triangle method.\"\"\"\n",
    "    ret, binary = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_TRIANGLE)\n",
    "    print(f\"Threshold value: {ret}\")\n",
    "    return binary\n",
    "\n",
    "def morphological_opening(binary_image, kernel_size=(5, 5), iterations=3):\n",
    "    \"\"\"Apply morphological opening to remove noise.\"\"\"\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    return cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel, iterations=iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c5c0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple Detection Functions\n",
    "\n",
    "def detect_red_objects(image):\n",
    "    \"\"\"Detect red objects (apples) in the image using color thresholding.\"\"\"\n",
    "    # Define red color range in BGR\n",
    "    lower_red = np.array([0, 0, 100])\n",
    "    upper_red = np.array([100, 100, 255])\n",
    "    \n",
    "    # Create mask for red objects\n",
    "    mask = cv2.inRange(image, lower_red, upper_red)\n",
    "    \n",
    "    # Apply mask to extract red objects\n",
    "    red_objects = cv2.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    return red_objects, mask\n",
    "\n",
    "def enhance_red_saturation(image, red_factor=1.5):\n",
    "    \"\"\"Enhance red saturation to improve apple detection.\"\"\"\n",
    "    # Convert to HSV for better color manipulation\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Create mask for red pixels\n",
    "    lower_red1 = np.array([0, 50, 50])\n",
    "    upper_red1 = np.array([10, 255, 255])\n",
    "    lower_red2 = np.array([170, 50, 50])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "    \n",
    "    mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "    red_mask = cv2.bitwise_or(mask1, mask2)\n",
    "    \n",
    "    # Enhance saturation for red pixels\n",
    "    hsv[red_mask > 0, 1] = np.clip(hsv[red_mask > 0, 1] * red_factor, 0, 255)\n",
    "    \n",
    "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "365ff897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple Counting and Location Functions\n",
    "\n",
    "def detect_apple_contours(processed_image, min_area=100):\n",
    "    \"\"\"Detect apple contours and return count with coordinates.\"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if len(processed_image.shape) == 3:\n",
    "        gray = cv2.cvtColor(processed_image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = processed_image\n",
    "    \n",
    "    # Find contours\n",
    "    contours, hierarchy = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    apple_info = []\n",
    "    apple_count = 0\n",
    "    \n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > min_area:  # Filter out small noise\n",
    "            # Calculate center coordinates\n",
    "            M = cv2.moments(cnt)\n",
    "            if M['m00'] != 0:  # Avoid division by zero\n",
    "                center_x = int(M['m10'] / M['m00'])\n",
    "                center_y = int(M['m01'] / M['m00'])\n",
    "                \n",
    "                # Get bounding rectangle\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                \n",
    "                apple_info.append({\n",
    "                    'id': apple_count + 1,\n",
    "                    'center_x': center_x,\n",
    "                    'center_y': center_y,\n",
    "                    'area': area,\n",
    "                    'bbox': (x, y, w, h)\n",
    "                })\n",
    "                apple_count += 1\n",
    "    \n",
    "    return apple_count, apple_info, contours\n",
    "\n",
    "def draw_apple_detection(image, apple_info, contours):\n",
    "    \"\"\"Draw detection results on the image.\"\"\"\n",
    "    result_image = image.copy()\n",
    "    \n",
    "    for i, info in enumerate(apple_info):\n",
    "        # Draw bounding rectangle\n",
    "        x, y, w, h = info['bbox']\n",
    "        cv2.rectangle(result_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw center point\n",
    "        cv2.circle(result_image, (info['center_x'], info['center_y']), 3, (0, 0, 255), -1)\n",
    "        \n",
    "        # Add text label\n",
    "        cv2.putText(result_image, f\"Apple {info['id']}\", \n",
    "                   (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "    \n",
    "    return result_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4201cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Processing Pipeline\n",
    "\n",
    "def process_single_image(image_path, visualize=True):\n",
    "    \"\"\"Process a single image to detect and count apples.\"\"\"\n",
    "    # Read image\n",
    "    original_image = read_image(image_path)\n",
    "    print(f\"Processing: {image_path.name}\")\n",
    "    \n",
    "    # Step 1: Enhance red saturation\n",
    "    enhanced_image = enhance_red_saturation(original_image)\n",
    "    \n",
    "    # Step 2: Apply Gaussian blur\n",
    "    blurred_image = gaussian_blur(enhanced_image)\n",
    "    \n",
    "    # Step 3: Detect red objects\n",
    "    red_objects, red_mask = detect_red_objects(blurred_image)\n",
    "    \n",
    "    # Step 4: Apply morphological operations\n",
    "    opened_mask = morphological_opening(red_mask)\n",
    "    \n",
    "    # Step 5: Detect apple contours and count\n",
    "    apple_count, apple_info, contours = detect_apple_contours(opened_mask)\n",
    "    \n",
    "    # Step 6: Draw detection results\n",
    "    result_image = draw_apple_detection(original_image, apple_info, contours)\n",
    "    \n",
    "    if visualize:\n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        axes[0, 0].imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "        axes[0, 0].set_title('Original Image')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(cv2.cvtColor(enhanced_image, cv2.COLOR_BGR2RGB))\n",
    "        axes[0, 1].set_title('Enhanced Red Saturation')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        axes[0, 2].imshow(cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB))\n",
    "        axes[0, 2].set_title('Gaussian Blur')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        axes[1, 0].imshow(red_mask, cmap='gray')\n",
    "        axes[1, 0].set_title('Red Object Mask')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(opened_mask, cmap='gray')\n",
    "        axes[1, 1].set_title('Morphological Opening')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        axes[1, 2].imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "        axes[1, 2].set_title(f'Result: {apple_count} Apples')\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'image_name': image_path.name,\n",
    "        'apple_count': apple_count,\n",
    "        'apple_locations': apple_info,\n",
    "        'result_image': result_image\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc676b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No images found in the input directory!\n"
     ]
    }
   ],
   "source": [
    "# Test on a Single Image\n",
    "\n",
    "# Get the first available image for testing\n",
    "test_images = list(INPUT_DIR.glob(\"*.jpg\"))\n",
    "if test_images:\n",
    "    test_image_path = test_images[0]  # Use first image for testing\n",
    "    print(f\"Testing on: {test_image_path}\")\n",
    "    \n",
    "    # Process the test image\n",
    "    result = process_single_image(test_image_path, visualize=True)\n",
    "    \n",
    "    print(f\"\\nResults for {result['image_name']}:\")\n",
    "    print(f\"Total apples detected: {result['apple_count']}\")\n",
    "    print(\"\\nApple locations:\")\n",
    "    for apple in result['apple_locations']:\n",
    "        print(f\"  Apple {apple['id']}: Center({apple['center_x']}, {apple['center_y']}), Area: {apple['area']}\")\n",
    "else:\n",
    "    print(\"No images found in the input directory!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4a6e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No images found!\n"
     ]
    }
   ],
   "source": [
    "# Batch Processing Function\n",
    "\n",
    "def process_all_images(max_images=10, save_results=True):\n",
    "    \"\"\"Process multiple images and collect results.\"\"\"\n",
    "    image_files = list(INPUT_DIR.glob(\"*.jpg\"))[:max_images]  # Limit for testing\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No images found!\")\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(image_files)} images...\")\n",
    "    \n",
    "    for image_path in image_files:\n",
    "        try:\n",
    "            result = process_single_image(image_path, visualize=False)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Save result image if requested\n",
    "            if save_results:\n",
    "                output_path = OUTPUT_DIR / f\"result_{result['image_name']}\"\n",
    "                cv2.imwrite(str(output_path), result['result_image'])\n",
    "            \n",
    "            print(f\"âœ“ {result['image_name']}: {result['apple_count']} apples\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error processing {image_path.name}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process a batch of images\n",
    "batch_results = process_all_images(max_images=5, save_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fed37ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results to summarize.\n"
     ]
    }
   ],
   "source": [
    "# Create Summary Statistics\n",
    "\n",
    "if batch_results:\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for result in batch_results:\n",
    "        summary_data.append({\n",
    "            'Image': result['image_name'],\n",
    "            'Apple_Count': result['apple_count'],\n",
    "            'Total_Area': sum([apple['area'] for apple in result['apple_locations']]),\n",
    "            'Avg_Apple_Area': np.mean([apple['area'] for apple in result['apple_locations']]) if result['apple_locations'] else 0\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"=== APPLE DETECTION SUMMARY ===\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Save summary to Excel\n",
    "    summary_path = OUTPUT_DIR / \"apple_detection_summary.xlsx\"\n",
    "    summary_df.to_excel(summary_path, index=False)\n",
    "    print(f\"\\nSummary saved to: {summary_path}\")\n",
    "    \n",
    "    # Display statistics\n",
    "    print(f\"\\n=== STATISTICS ===\")\n",
    "    print(f\"Total images processed: {len(batch_results)}\")\n",
    "    print(f\"Total apples detected: {summary_df['Apple_Count'].sum()}\")\n",
    "    print(f\"Average apples per image: {summary_df['Apple_Count'].mean():.2f}\")\n",
    "    print(f\"Max apples in single image: {summary_df['Apple_Count'].max()}\")\n",
    "    print(f\"Min apples in single image: {summary_df['Apple_Count'].min()}\")\n",
    "else:\n",
    "    print(\"No results to summarize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fc571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
